<center><b>Security and Privacy in Machine Learning</b></center>
<center>Sharif University of Technology, Iran</center>
<center>CE Department</center>
<center>Fall 2025</center>


&nbsp;&nbsp;&nbsp;

v
_Welcome_ to the public page for the course on Security and Privacy in Machine Learning (SPML). The main objectives of the course are to introduce students to the principles of security and privacy in machine learning. The students become familiar with the vulnerabilities of machine learning in the training and inference phases and the methods to improve the robustness and privacy of machine learning models.



**Course Logistics**

   * **Time:** -
   * **Location:** - & [vc.sharif.edu/ch/amsadeghzadeh](https://vc.sharif.edu/ch/amsadeghzadeh)
   * **Contact:** Announcements and all course-related questions will happen on the [Quera](https://quera.org/course/add_to_course/course/18760/) forum. 
    * All official announcements and communication will happen over [Telegram](https://t.me/SPML2025) channel.
     * For external enquiries, emergencies, or personal matters that you don't wish to put in a private post, you can email me at sadeghzadeh_at_sharif_dot_edu



**Instructor**

&nbsp;&nbsp;&nbsp;_Amir Mahdi Sadeghzadeh_  
&nbsp;&nbsp;&nbsp;Office: CE-704
&nbsp;&nbsp;&nbsp;Lab: CE-502
&nbsp;&nbsp;&nbsp;Office Hours: By appointment (through Email)  
&nbsp;&nbsp;&nbsp;Email: [amsadeghzadeh_at_gmail.com](mailto:amsadeghzadeh@gmail.com)  
&nbsp;&nbsp;&nbsp;URL: [amsadeghzadeh.github.io](https://amsadeghzadeh.github.io)  



**Course Staff**

* _Arian Komaei Koma_ (Head Course Assistant) - Email: [ariankomaei_at_gmail.com](mailto:ariankomaei@gmail.com)
* _Alireza Faraj Tabrizi_ (Head Course Assistant) - Email: [Alireza15farajtabrizi_at_gmail.com](mailto:Alireza15farajtabrizi@gmail.com)
* _Amir Ezzati_ (Head Course Assistant) - Email: [iamirezzati_at_gmail.com](mailto:iamirezzati@gmail.com)
* _Firoozeh Abrishami_ (Course Assistant) - Email: [f.abrishami110_at_gmail.com](mailto:f.abrishami110@gmail.com)
* _Ramtin moslemi_ (Course Assistant) - Email: [ramtin4moslemi_at_gmail.com](mailto:ramtin4moslemi@gmail.com)
* _Erfan Sobhaei_ (Course Assistant) - Email: [E.sobhaei_at_gmail.com](mailto:E.sobhaei@gmail.com)





**Course Pages** 

* [spml2025.github.io](spml2025.github.io) -> Course information, syllabus, and materials.
* [Course Telgram Channel](https://t.me/SPML2025)
* [Quera](https://quera.org/course/add_to_course/course/18760/) (Get the password from course staff) -> Announcements, assignments, and all course-related questions.
* [Course Telegram Group](https://t.me/kp_gfe) -> Contact the Head Teaching Assistant for student confirmation and group inclusion.


**Main References** 

The main references for the course are many research papers in top-tier conferences and journals in computer security (SP, CCS, Usenix Security, EuroSP) and machine learning (NeurIPS, ICLR, ICML, CVPR, ECCV). Three following books are used for presenting background topics in machine learning and deep learning in
the first part of the course.

-   [Christopher M. Bishop, *Pattern Recognition and Machine Learning*,
    Springer,
    2006.](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)

-   [Ian Goodfellow, *Deep Learning*, MIT Press,
    2016.](https://www.deeplearningbook.org/)

-   [Aston Zhang, Dive into Deep Learning, 2020 ](http://d2l.ai/)



**Grading Policy**

- Assignments (30%) 

- 4 Quizzes (10%)

- Presentation (10%)

- Mid-term (20%)

- Final (30%).



**Course Policy**

-   This course considers topics involving personal and public privacy
    and security. As part of this investigation we will cover
    technologies whose abuse may infringe on the rights of others. As an
    instructor, I rely on the ethical use of these technologies.
    Unethical use may include circumvention of existing security or
    privacy measurements for any purpose, or the dissemination,
    promotion, or exploitation of vulnerabilities of these services.
    Exceptions to these guidelines may occur in the process of reporting
    vulnerabilities through public and authoritative channels. Any
    activity outside the letter or spirit of these guidelines will be
    reported to the proper authorities and may result in dismissal from
    the class. When in doubt, please contact the instructor for advice. **Do not**
    undertake any action which could be perceived as technology misuse
    anywhere and/or under any circumstances unless you have received
    explicit permission from Dr. Sadeghzadeh.



**Academic Honesty** 

[Sharif CE Department Honor Code](https://wiki.ce.sharif.edu/%D8%A2%DB%8C%DB%8C%D9%86_%D9%86%D8%A7%D9%85%D9%87/%D8%A2%D8%AF%D8%A7%D8%A8_%D9%86%D8%A7%D9%85%D9%87_%D8%A7%D9%86%D8%AC%D8%A7%D9%85_%D8%AA%D9%85%D8%B1%DB%8C%D9%86_%D9%87%D8%A7%DB%8C_%D8%AF%D8%B1%D8%B3%DB%8C) (please read it carefully!)



**Homework Submission**

Submit your answers in .pdf or .zip file in course page on Quera website, with the following format:
HW[HW#]-[FamilyName]-[std#] (For example HW3-Hoseini-401234567)



**Late Policy**

* All students have 14 free late days for the assignments.
* You may use up to 5 late days per assignment with no penalty.
* Once you have exhausted your free late days, we will deduct a late penalty of 20% per additional late day.


&nbsp;&nbsp;&nbsp;

&nbsp;&nbsp;&nbsp;


| # | Date  | Topic             | Content                                    | Lecture | Reading                                                                                                                                                                                                                                                                               | HWs | Quiz
|---|-------|-------------------|--------------------------------------------|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------|------|
| 1 | 7/19 | Course Intro.     | The scope and contents of the course       | [Lec1](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec1.pdf)    | [Towards the Science of Security and Privacy in Machine Learning](https://arxiv.org/abs/1611.03814)                 |          |                                                                                                                                                 |     |
| 2 | 7/21 | Deep Learning Review     | ML Intro., Perceptron, Logistic regression, GD, Regularization       | [Lec2](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec2.pdf)    | [Pattern Recognition and Machine Learning Ch.1 & Ch.4](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf) <br> [Deep Learning Ch.5 & Ch.6](https://www.deeplearningbook.org/)                                                                                                                                                                          |     |
| 3 | 7/26 | Adversarial Examples     | Properties of neural networks, Adversarial Example (L-BFGS), Type of Adversarial Attacks            | [Lec3](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec3.pdf)    |  [Intriguing Properties of Neural Networks](https://arxiv.org/abs/1312.6199)                 |     |
| 4 | 7/28 | Adversarial Examples    | Fast Gradient Sign Method(FGSM) Attack, L_P Norms         | [Lec4](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec4.pdf)    | [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)     |                                                                               |     |
| 5 | 8/3 | Adversarial Examples             |  C&W Attack                      | [Lec5](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec5.pdf)   | [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644) |     | 
| 6 | 8/5  | Adversarial Examples             |   Projected Gradient Descent, Universal Adversarial Perturbations, Adversarial Patch                           | [Lec6](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec6.pdf)   |      [Universal Adversarial Perturbations](https://arxiv.org/abs/1610.08401) <br> [Adversarial Patch](https://arxiv.org/abs/1712.09665)              |      |  
| 7 | 8/10 | Adversarial Examples             |Obfuscated Gradients, Adversarial Training, TRADES                        | [Lec7](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec7.pdf)   |   [Theoretically Principled Trade-off between Robustness and Accuracy](https://arxiv.org/abs/1901.08573)  <br>   [Mitigating Adversarial Effects Through Randomization](https://arxiv.org/abs/1711.01991) <br> [Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks](https://arxiv.org/abs/1511.04508)|    | 
| 8 | 8/12 | Adversarial Examples              | Certifiable Robustness, Randomized Smoothing                       | [Lec8](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec8.pdf)   |[Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/abs/1902.02918)|    |
| 9 | 8/17 | Black Box AE             |                       | [Lec9](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec9.pdf)   |                                                                   |    |
<!-- | 10 | 8/19 |           |                    |   |                                                            |    | 
| 11 | 8/24 | Adversarial Examples             |      Defenses Against AEs             | [Lec10](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec10.pdf)   |  [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/pdf/1902.02918) <br> [Provably robust deep learning via adversarially trained smoothed classifiers](https://proceedings.neurips.cc/paper/2019/file/3a24b25a7b092a252166a1641ae953e7-Paper.pdf)                                                            |    |
| 12 | 8/26 | Adversarial Examples             |      Defenses Against AEs                  | [Lec11](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec11.pdf)   |   [Certified Adversarial Robustness via Randomized Smoothing](https://arxiv.org/pdf/1902.02918) <br> [Provably robust deep learning via adversarially trained smoothed classifiers](https://proceedings.neurips.cc/paper/2019/file/3a24b25a7b092a252166a1641ae953e7-Paper.pdf) <br> [Practical Black-Box Attacks against Machine Learning](https://www.cs.purdue.edu/homes/bb/2020-fall-cs590bb/docs/at/attacks-against-machine-learning.pdf)                                                          |    | [Quiz2]()
| 13 | 9/1   |      Adversarial Examples             |       Black-box AEs                                     |   [Lec12](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec12.pdf)      |      [ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models](https://dl.acm.org/doi/pdf/10.1145/3128572.3140448)  <br> [Black-box Adversarial Attacks with Limited Queries and Information](https://arxiv.org/pdf/1804.08598)  | |
| E | 9/6   |      Midterm             |                                         |        |        |  |
| 14 | 9/8   |      Adversarial Examples             |       Black-box AEs   - Data Poisoning                                 |   [Lec13](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec13.pdf)    |    [Black-box Adversarial Attacks with Limited Queries and Information](https://arxiv.org/pdf/1804.08598) <br> [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733)  <br> [Clean-Label Backdoor Attacks](https://people.csail.mit.edu/madry/lab/cleanlabel.pdf)   |  |
| 15 | 9/10  | Poisoning                        | Poisoning                                  |  [Lec14](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec14.pdf)  | [Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks](https://arxiv.org/abs/1804.00792) <br> [Deep Partition Aggregation: Provable Defense against General Poisoning Attacks](https://arxiv.org/pdf/2006.14768)  ||
| 16 | 9/15  | Model Extraction                 | ME Attacks                                 | [Lec15](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec15.pdf)    | [High Accuracy and High Fidelity Extraction of Neural Networks](https://arxiv.org/abs/1909.01838)  <br> [Knockoff Nets: Stealing Functionality of Black-Box Models](https://arxiv.org/abs/1812.02766) |     |
| 17 | 9/17  | Model Extraction   - Privacy              | ME Defenses - Privacy Risks                              | [Lec16](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec16.pdf)    |  [Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring](https://arxiv.org/pdf/1802.04633)                     |     |                                                                                                       
| 18 | 9/22  | Privacy                          | Privacy Risks                       | [Lec17](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec17.pdf)    | [Membership Inference Attacks against Machine Learning Models](https://arxiv.org/abs/1610.05820)                                                                                                                    |    |   [Quiz3]()
| 19 | 9/24   | Presentation                     |            Student presentation                                |        |     |     |
| 20 | 9/29   | Presentation                     |            Student presentation                                |        |     |     |
| 21 | 10/1   | Presentation                     |            Student presentation                                |        |     |     |
| 22 | 10/6  | Privacy                          | Differential Privacy                       | [Lec18](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec18.pdf)   |  [Passive and Active White-box Inference Attacks against Centralized and Federated Learning](https://arxiv.org/abs/1812.00910)                                                                                                                                                                                                                                                        |     |
| 23 | 10/8   | Privacy                          | Privacy-preserving DL                      | [Lec19](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec19.pdf)   |  [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)                                                                                                                                                                                                       |     |                                                                                                                         |     |
| 24 | 10/15   | Privacy                          | Privacy-preserving DL                      | [Lec20](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec20.pdf)   | [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)  <br> [Deep Learning with Differential Privacy](https://arxiv.org/abs/1607.00133)                                                                                                                                                                                                  |     |                                                                                                                         |     |
| 25 | 10/20   | Privacy                          | Privacy-preserving DL                      | [Lec20](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec20.pdf)   | [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)  <br> [Deep Learning with Differential Privacy](https://arxiv.org/abs/1607.00133) <br> [Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data](https://arxiv.org/abs/1610.05755)                                                                                                                                                                                                       |     |                                                                                                                         |     |
| 26 | 10/22   | Generative models security                         |                     |   [Lec21](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec21.pdf) |                                                                                                                                                                                                        |     |                                                                                                                         |     |
| 27 | 10/27   | Generative models security                           |                     | [Lec22](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec22.pdf)  |                                                                                                                                                                                                      |     |                                                                                                                         |     |
| 26 | 10/29   | Generative models security                         |                     | [Lec23](https://github.com/spml2025/spml2025.github.io/raw/main/Lectures/Lec23.pdf)   |                                                                                                                                                                                                        |     |                                                                                                                         |     |
-->
